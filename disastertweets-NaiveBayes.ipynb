{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pyspellchecker\n",
    "!nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "53bead4f-4efb-45bf-b98a-6bf6098b402f",
    "_uuid": "1b24d9a4-f945-40ad-b053-e6b13779ce2e",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import cpu_count\n",
    "from multiprocessing import Process, Queue\n",
    "from spellchecker import SpellChecker\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PROCESSES = cpu_count()\n",
    "STEMMER = WordNetLemmatizer()\n",
    "SPELL_CHECKER = SpellChecker()\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_skewness(df, cols, threshold=1):\n",
    "    skewed_cols = []\n",
    "    for col in cols:\n",
    "        if abs(df[col].skew()) >= threshold:\n",
    "            skewed_cols.append(col)\n",
    "            \n",
    "    return skewed_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_junk_words = [\"\\x89Û_\",\"\\x89ÛÒ\",\"\\x89Û÷\",\"&gt;\",\"%20\",\"\\x89Ûª\",\"&lt;\",\"\\x89ÛÏ\",\"\\x89Û\\x9d\",\"åÊ\",\"&amp;\",\"\\n\"]\n",
    "custom_junk_words = \"|\".join(custom_junk_words)\n",
    "custom_junk_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    #Remove account tags\n",
    "    ac_tags = re.compile(r\"@\\S+\")\n",
    "    text = ac_tags.sub(\"\", text)\n",
    "    \n",
    "    #Remove Junk \n",
    "    text = re.sub(custom_junk_words,\" \",text)\n",
    "    \n",
    "    # Removing URLs\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    text = url.sub(\"\", text)\n",
    "    \n",
    "    # Removing HTML tags & contents inside it\n",
    "    html=re.compile(\"<.*?>.*</?.*?>\")\n",
    "    text = html.sub(\"\", text)\n",
    "    \n",
    "    # Removing non-alpha characters\n",
    "    punct = re.compile(r\"[^a-zA-Z\\s]\")\n",
    "    text = punct.sub(\"\", text)\n",
    "    \n",
    "    # Remove single and double lettered words\n",
    "    text = ' '.join([('' if len(word)<=3 else word) for word in text.split(' ')]).strip()\n",
    "    \n",
    "    # Remove extra white spaces\n",
    "    extra_white_spaces = re.compile(\"\\s{2,}\")\n",
    "    text = extra_white_spaces.sub(\" \", text)\n",
    "    \n",
    "    \n",
    "    # Correcting misspelled words\n",
    "    splitted_text = text.split()\n",
    "    misspelled_words = set(SPELL_CHECKER.unknown(splitted_text))\n",
    "    if len(misspelled_words) > 0:\n",
    "        corrected_text = []\n",
    "        for word in splitted_text:\n",
    "            if word in misspelled_words:\n",
    "                corrected_text.append(SPELL_CHECKER.correction(word))\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "        text = \" \".join(corrected_text)\n",
    "    \n",
    "        #     Remove Stopwords and Lemmatize\n",
    "    text = \" \".join([STEMMER.lemmatize(token) for token in text.lower().split() if token not in STOP_WORDS])\n",
    "    \n",
    "        # Remove single and double lettered words\n",
    "    text = ' '.join([('' if len(word)<=3 else word) for word in text.split(' ')]).strip()\n",
    "    text = extra_white_spaces.sub(\" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessConcurrent:\n",
    "    def __init__(self, chunk_size=100):\n",
    "        self.chunk_size = chunk_size\n",
    "    \n",
    "    def preprocess_texts(self, text_series):\n",
    "        start_chunk_idx, end_chunk = 0, text_series.shape[0]\n",
    "        processes, q = list(), Queue()\n",
    "        while start_chunk_idx < end_chunk:\n",
    "            last_chunk_start_idx, cur_process_idx = start_chunk_idx, 1\n",
    "            while cur_process_idx <= N_PROCESSES and start_chunk_idx < end_chunk:\n",
    "                if start_chunk_idx+self.chunk_size <= end_chunk:\n",
    "                    p = Process(target=self._preprocess_text_parallel, args=(\n",
    "                        text_series[start_chunk_idx:start_chunk_idx+self.chunk_size], q\n",
    "                    ))\n",
    "                else:\n",
    "                    p = Process(target=self._preprocess_text_parallel, args=(\n",
    "                        text_series[start_chunk_idx:end_chunk], q\n",
    "                    ))\n",
    "                p.start()\n",
    "                processes.append(p)\n",
    "                cur_process_idx += 1\n",
    "                start_chunk_idx += self.chunk_size\n",
    "            \n",
    "            print(f\"Currently processing chunks from {last_chunk_start_idx} to {start_chunk_idx} \"\n",
    "                  f\"out of total {end_chunk} chunks\")\n",
    "            while len(processes) > 0:\n",
    "                p = processes.pop()\n",
    "                p.join()\n",
    "\n",
    "            while not q.empty():\n",
    "                start, end, preprocessed_text_series = q.get()\n",
    "                text_series[start:end].update(preprocessed_text_series)\n",
    "                \n",
    "        q.close()\n",
    "    \n",
    "    def _preprocess_text_parallel(self, texts, q):\n",
    "        for idx, text in texts.items():\n",
    "            texts[idx] = preprocess_text(text)\n",
    "\n",
    "        q.put([texts.index[0], texts.index[-1], texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n",
    "train_labels = train_df[\"target\"]\n",
    "train_df.drop(columns=[\"target\"], inplace=True)\n",
    "print(f\"Training shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "concurrency = PreprocessConcurrent()\n",
    "print(\"Original:\\n\", train_df.text[10:20])\n",
    "concurrency.preprocess_texts(train_df.text)\n",
    "print(\"Processed:\\n\", train_df.text[10:20])\n",
    "print(\"--- %s Minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=2000, strip_accents=\"ascii\",\n",
    "    ngram_range=(1,1),max_df=0.9\n",
    ")\n",
    "X = vectorizer.fit_transform(train_df.text).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gaussian_nb = GaussianNB()\n",
    "scores = cross_val_score(gaussian_nb, X, train_labels, scoring=\"f1\",n_jobs=-1)\n",
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_df.text).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Original:\\n\", test_df.text[10:20])\n",
    "concurrency.preprocess_texts(test_df.text)\n",
    "print(\"Processed:\\n\", test_df.text[10:20])\n",
    "X_test = vectorizer.transform(test_df.text).todense()\n",
    "print(\"--- %s Minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb.fit(X, train_labels)\n",
    "submission = pd.read_csv(\"Data/sample_submission.csv\")\n",
    "submission[\"target\"] = gaussian_nb.predict(X_test)\n",
    "submission.to_csv(\"result.csv\", index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
