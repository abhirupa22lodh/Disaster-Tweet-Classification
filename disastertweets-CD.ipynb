{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Downloading packages","metadata":{}},{"cell_type":"code","source":"!pip3 install pyspellchecker\n!nltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:29:13.789517Z","iopub.execute_input":"2021-10-11T07:29:13.789814Z","iopub.status.idle":"2021-10-11T07:29:27.024959Z","shell.execute_reply.started":"2021-10-11T07:29:13.789784Z","shell.execute_reply":"2021-10-11T07:29:27.024119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import re\nimport spacy\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom spellchecker import SpellChecker\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n%matplotlib inline","metadata":{"_uuid":"1b24d9a4-f945-40ad-b053-e6b13779ce2e","_cell_guid":"53bead4f-4efb-45bf-b98a-6bf6098b402f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-10-11T07:29:27.027116Z","iopub.execute_input":"2021-10-11T07:29:27.027371Z","iopub.status.idle":"2021-10-11T07:29:29.292293Z","shell.execute_reply.started":"2021-10-11T07:29:27.027339Z","shell.execute_reply":"2021-10-11T07:29:29.29157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Global Variables","metadata":{}},{"cell_type":"code","source":"STEMMER = WordNetLemmatizer()\nSPELL_CHECKER = SpellChecker()\nSTOP_WORDS = set(stopwords.words(\"english\"))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:29:29.293281Z","iopub.execute_input":"2021-10-11T07:29:29.293972Z","iopub.status.idle":"2021-10-11T07:29:29.454725Z","shell.execute_reply.started":"2021-10-11T07:29:29.29394Z","shell.execute_reply":"2021-10-11T07:29:29.454032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility Functions","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text):\n    # Removing URLs\n    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n    text = url.sub(\"\", text)\n    \n    # Removing HTML tags & contents inside it\n    html=re.compile(\"<.*?>.*</?.*?>\")\n    text = html.sub(\"\", text)\n    \n    # Removing non-alpha characters\n    punct = re.compile(r\"[^a-zA-Z\\s']\")\n    text = punct.sub(\"\", text)\n    \n    # Remove extra white spaces\n    extra_white_spaces = re.compile(\"\\s{2,}\")\n    text = extra_white_spaces.sub(\" \", text)\n    \n    # Correcting misspelled words\n    splitted_text = text.split()\n    misspelled_words = set(SPELL_CHECKER.unknown(splitted_text))\n    if len(misspelled_words) > 0:\n        corrected_text = []\n        for word in splitted_text:\n            if word in misspelled_words:\n                corrected_text.append(SPELL_CHECKER.correction(word))\n            else:\n                corrected_text.append(word)\n        text = \" \".join(corrected_text)\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:29:29.456508Z","iopub.execute_input":"2021-10-11T07:29:29.456767Z","iopub.status.idle":"2021-10-11T07:29:29.466279Z","shell.execute_reply.started":"2021-10-11T07:29:29.456737Z","shell.execute_reply":"2021-10-11T07:29:29.465244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_text(text):\n    return([STEMMER.lemmatize(token) for token in text.lower().split() if token not in STOP_WORDS])","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:29:29.467776Z","iopub.execute_input":"2021-10-11T07:29:29.468136Z","iopub.status.idle":"2021-10-11T07:29:29.477423Z","shell.execute_reply.started":"2021-10-11T07:29:29.468094Z","shell.execute_reply":"2021-10-11T07:29:29.476574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing dataset & EDA","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntrain_labels = train_df[\"target\"]\ntrain_df.drop(columns=[\"target\"], inplace=True)\nprint(f\"Training shape: {train_df.shape}\")\nprint(f\"Test shape: {test_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:29:41.562114Z","iopub.execute_input":"2021-10-11T07:29:41.562449Z","iopub.status.idle":"2021-10-11T07:29:41.649669Z","shell.execute_reply.started":"2021-10-11T07:29:41.562418Z","shell.execute_reply":"2021-10-11T07:29:41.648561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature generations","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n    preprocessor=preprocess_text, tokenizer=tokenize_text, max_df=0.7, min_df=0.2, strip_accents=\"ascii\", \n)\nvectorizer.fit(train_df.text)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:29:46.11741Z","iopub.execute_input":"2021-10-11T07:29:46.117854Z","iopub.status.idle":"2021-10-11T07:46:36.553439Z","shell.execute_reply.started":"2021-10-11T07:29:46.117825Z","shell.execute_reply":"2021-10-11T07:46:36.552143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(f\"Vocabukary size is: {vectorizer.vocabulary_.keys()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}