{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Downloading packages","metadata":{}},{"cell_type":"code","source":"!pip3 install pyspellchecker\n!nltk.download('wordnet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import re\nimport spacy\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom os import cpu_count\nfrom multiprocessing import Process, Queue\nfrom spellchecker import SpellChecker\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\n%matplotlib inline","metadata":{"_uuid":"1b24d9a4-f945-40ad-b053-e6b13779ce2e","_cell_guid":"53bead4f-4efb-45bf-b98a-6bf6098b402f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Global Variables","metadata":{}},{"cell_type":"code","source":"N_PROCESSES = cpu_count()\nSTEMMER = WordNetLemmatizer()\nSPELL_CHECKER = SpellChecker()\nSTOP_WORDS = set(stopwords.words(\"english\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility Functions","metadata":{}},{"cell_type":"code","source":"def cal_skewness(df, cols, threshold=1):\n    skewed_cols = []\n    for col in cols:\n        if abs(df[col].skew()) >= threshold:\n            skewed_cols.append(col)\n            \n    return skewed_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    # Removing URLs\n    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n    text = url.sub(\"\", text)\n    \n    # Removing HTML tags & contents inside it\n    html=re.compile(\"<.*?>.*</?.*?>\")\n    text = html.sub(\"\", text)\n    \n    # Removing non-alpha characters\n    punct = re.compile(r\"[^a-zA-Z\\s']\")\n    text = punct.sub(\"\", text)\n    \n    # Remove extra white spaces\n    extra_white_spaces = re.compile(\"\\s{2,}\")\n    text = extra_white_spaces.sub(\" \", text)\n    \n    # Correcting misspelled words\n    splitted_text = text.split()\n    misspelled_words = set(SPELL_CHECKER.unknown(splitted_text))\n    if len(misspelled_words) > 0:\n        corrected_text = []\n        for word in splitted_text:\n            if word in misspelled_words:\n                corrected_text.append(SPELL_CHECKER.correction(word))\n            else:\n                corrected_text.append(word)\n        text = \" \".join(corrected_text)\n\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PreprocessConcurrent:\n    def __init__(self, chunk_size=100):\n        self.chunk_size = chunk_size\n    \n    def preprocess_texts(self, text_series):\n        start_chunk_idx, end_chunk = 0, text_series.shape[0]\n        processes, q = list(), Queue()\n        while start_chunk_idx < end_chunk:\n            last_chunk_start_idx, cur_process_idx = start_chunk_idx, 1\n            while cur_process_idx <= N_PROCESSES and start_chunk_idx < end_chunk:\n                if start_chunk_idx+self.chunk_size <= end_chunk:\n                    p = Process(target=self._preprocess_text_parallel, args=(\n                        text_series[start_chunk_idx:start_chunk_idx+self.chunk_size], q\n                    ))\n                else:\n                    p = Process(target=self._preprocess_text_parallel, args=(\n                        text_series[start_chunk_idx:end_chunk], q\n                    ))\n                p.start()\n                processes.append(p)\n                cur_process_idx += 1\n                start_chunk_idx += self.chunk_size\n            \n            print(f\"Currently processing chunks from {last_chunk_start_idx} to {start_chunk_idx} \"\n                  f\"out of total {end_chunk} chunks\")\n            while len(processes) > 0:\n                p = processes.pop()\n                p.join()\n\n            while not q.empty():\n                start, end, preprocessed_text_series = q.get()\n                text_series[start:end].update(preprocessed_text_series)\n                \n        q.close()\n    \n    def _preprocess_text_parallel(self, texts, q):\n        for idx, text in texts.items():\n            texts[idx] = preprocess_text(text)\n\n        q.put([texts.index[0], texts.index[-1], texts])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_text(text):\n    return([STEMMER.lemmatize(token) for token in text.lower().split() if token not in STOP_WORDS])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing dataset & EDA","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntrain_labels = train_df[\"target\"]\ntrain_df.drop(columns=[\"target\"], inplace=True)\nprint(f\"Training shape: {train_df.shape}\")\nprint(f\"Test shape: {test_df.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature generations","metadata":{}},{"cell_type":"code","source":"concurrency = PreprocessConcurrent()\nprint(\"Original:\\n\", train_df.text[10:20])\nconcurrency.preprocess_texts(train_df.text)\nprint(\"Processed:\\n\", train_df.text[10:20])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n    tokenizer=tokenize_text, max_features=1000, strip_accents=\"ascii\", \n)\nX = vectorizer.fit_transform(train_df.text).todense()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Gaussian Naive Bayes\ngaussian_nb = GaussianNB()\nscores = cross_val_score(gaussian_nb, X, train_labels, scoring=\"f1\")\nplt.plot(scores)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nparam_grid = {\"penalty\": [\"l2\"], \"C\": [0.01, 1, 10], \"max_iter\": [100, 400, 800], \"solver\": [\"newton-cg\"]}\nlogistic_reg = GridSearchCV(LogisticRegression(), param_grid, scoring=\"f1\")\nlogistic_reg.fit(X, train_labels)\nprint(f\"Best score: {logistic_reg.best_score_}\")\nprint(f\"Best parameters: {logistic_reg.best_estimator_}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"print(\"Original:\\n\", test_df.text[10:20])\nconcurrency.preprocess_texts(test_df.text)\nprint(\"Processed:\\n\", test_df.text[10:20])\nX_test = vectorizer.transform(test_df.text).todense()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gaussian_nb.fit(X, train_labels)\nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsubmission[\"target\"] = gaussian_nb.predict(X_test)\nsubmission.to_csv(\"result.csv\", index=False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}